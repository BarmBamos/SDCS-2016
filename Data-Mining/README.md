# 数据挖掘
## 数据预处理

## 数据仓库和联机分析处理
### 数据仓库
#### 联机分析处理（OLAP）

##### OLAP基本操作
[多维数据上的典型OLAP操作](4-1.png)

1. 上卷（roll-up）

	通过沿一个维的概念分层向上攀升或者通过维规约在数据立方体上进行聚集。如location层被定义为全序street<city<province<country，对city做上卷操作，将会由city层向上到country层聚集数据，结果立方体按country而不是对city对数据分组。
	
2. 下钻（drill-down）

	下钻由不太详细的数据到更详细的数据。下钻可以沿维的概念分层向下或引入附加的维来实现，结果数据立方体会列出更为具体的数据分组。
	
3. 切片（slice）

	在给定的数据立方体的一个维上进行选择，导出一个子立方体。被选择的维上只有特定一种属性。

4. 切块（dice） 

	在两个或者多个维上进行选择，定义子立方体。

5. 转轴（pivot）

	转轴是一种目视操作，提供数据的替代表示。

## 关联和相关性


## 分类和预测
### 基本概念

分类是一种重要数据分析形式，它提取刻画重要数据类的模型，这种模型称为分类器，用来预测分类的类标号。举个简单例子来说，银行在对个人信息进行分析后，可以进行初步判断该客户贷款申请是安全的还是危险的。另一种方式是对连续值函数或有序值进行预测，这种模型是预测器。比如更具消费者日常消费习惯预测消费者在特定日期消费金额数。所以分类的实质其实是通过已有训练数据生成一种能够匹配这些数据的模型，进而用此模型将其他只含有数据库元组的数据对应到某一具体类标号中。

通常分类有两个基本步骤。

1. 建立分类模型（学习阶段）：
	
	从机器学习的角度看，分类是一种有指导的学习，即每个训练数据由数据库元组和关联的类标号组成，通过学习形成数据元组和类标识之间对应的知识。元组用n维向量表示，分别表示元组在n个数据库属性A<sub>1</sub>，A<sub>2</sub>，A<sub>3</sub>……A<sub>n</sub>上的n个度量，每个元组都有一个预先定义好的类标号c，则训练数据样本为X = (x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>……x<sub>n</sub>, c)。因为每个训练数据都提供了训练元组和类标号，这一阶段又叫做监督学习。
	
	分类过程在学习阶段可以看成是学习一个映射关系y=f(X)。通过训练集得到这一映射关系之后再将给定元组作为输入就可以得到X的类标号y。典型情况下该映射用分类规则、决策树或数学公式提供。

2. 使用模型为数据的类标号：

	在这一阶段我们首先需要做得是对模型的准确性进行评估。对数据进行评估需要采用检验集，检验集和训练集有着相同的结构，都是包含数据元组及类标识。将检验集的数据元组作为前一步骤生成的模型的输入部分，然后讲输出部分和检验集原本的类标识进行比较，以此来确定准确率。如果测试集得到的准确率较高，则可以用此模型来对数据进行分类。
	
除了以上两点，有监督学习还必须注意的地方就是过分拟合问题。考虑一种可能情况就是训练数据中含有少量严重失准的数据，如果模型对这部分数据也进行拟合，最终的结果可能会比较糟糕。
	
### 常见分类方法
#### 决策树归纳
决策树归纳是从训练元组中学习决策树。决策树内部节点（非树叶节点）表示在一个属性上的测试，每个分枝表示该测试的一个输出，每个输液节点存放一个类标号。

使用决策树时，给定一个类标号未知的元组X，在决策树上测试该元组的属性值。跟踪一条由根到叶节点的路径，该叶节点就存放着该元组类的预测。

决策树被用于分类有以下几点优势。

1. 不需要任何领域知识或参数设置。

2. 决策树可以处理高维数据。

3. 树形结构容易被人理解。

##### 决策树构造方法
[决策树构造步骤](8-1.png)

##### 属性选择度量
属性选择度量是选择分裂准则，把给定类标记的训练元组的数据分区D最好地划分成单独类的启发式方法，这里最好的表示分类准则导致的划分，划分后各个分区应当是纯的（落在一个给定分区的所有元组都属于相同的类）。每次具有最好度量得分的属性被选为给定原则的分裂属性，从准则的每个输出生长出分枝，并且相应地划分元组。下面是三种常用的属性选择度量。

1. 信息增益：

	ID3使用信息增益作为属性选择度量。

2. 增益率：

	C4.5使用增益率作为属性选择度量。

3. 基尼指数：

	基尼指数在CART中作为属性选择度量。

##### 树减枝


#### 贝叶斯分类

## 聚类分析

聚类是一个把对象集划分成多个组或簇的过程，使得簇内对象具有很高的相似性，但与其他簇中的对象很不相似。相似性和相异性的评估通常需要涉及距离，但基于距离的聚类算法往往发现的是球形的聚类，不能用于任何形状的簇。比如在图像识别应用中（识别数字），不同人写的数字具有不同的习惯，我们可以使用聚类确定某个数字的子类，每个子类代表可能出现的某个数字的变体，使用基于子类的多个模型可以提高整体识别的准确率。

目前聚类分析主要集中于距离的分析，最常用的几种包括k-均值（k-means）,k-中心点（k-medoids）等。在机器学习领域，分类称为监督学习，因为给定了类标号信息，即学习算法是监督的，因为它被告知每个训练元组的类隶属关系。聚类被称为无监督学习，因为没有提供类标号信息。

### 对聚类分析要求

1. 可伸缩性：

	能够对大量海量数据依旧具有较准确的结果。

2. 处理不同属性类型的能力：

	要能够处理非数值型数据，如二元的、对称的、序数的等。

3. 发现任意形状的簇：

	基于距离度量算法趋向于发现具有相近尺寸和密度的球状簇，要能处理任意形状的簇。

4. 对于确定输入参数的领域知识的要求：

	聚类算法需要提供的参数需要由用户提供，需要用户具有一定的专业基础，如果提供的参数值不当将严重影响结果。

5. 处理噪声数据的能力：

	现实情况下，数据中会包含离群值、缺损值或者错误数据，优秀的聚类算法要能够对这些无效数据不敏感。

6. 增量聚类对输入次序不敏感：

	- 增量聚类：许多情况下会有新数据产生，算法能够即使更新结果而不用从头重新开始聚类。
	
	- 次序不敏感：聚类算法对提供次序不同的数据都能得到近似结果。

7. 聚类高维数据能力：

	某些数据集可能包含大量的维和属性，如文档聚类时每个关键字都可以看成看做一个维，聚类算法要能够满足高维数据的处理，而不仅仅集中在二、三维数据。


### 常见划分方法
给定一个n个对象的集合，构建数据的k个分区(k<=n)，每个分区表示一个簇.簇内的元素相似，簇间元素相异。

#### k-均值（基于形心的技术）

k-均值算法流程如下：

1. 随机选择k个对象，每个对象代表一个簇的初始均值或中心。

2. 对剩余的每个对象，根据它与簇均值的距离，将他指派到最相似的簇。

3. 计算每个簇的新均值。

4. 回到步骤2，循环，直到准则函数收敛。   

优点：

1. 算法复杂度较低，为O(nkt)，其中n为对象总数，k是簇的个数，t是迭代次数。

2. 对大数据集，该算法是相对可伸缩的和有效的。

缺点：

1. 只有当簇均值有定义的情况下，k均值方法才能使用。

2. 不适合发现非凸形状的簇，或者大小差别很大的簇。

3. 用户必须首先给定簇数目。

4. 对噪声和离群点数据敏感。
 
   - 一个具有很大极端值的对象可能显著的扭曲数据的分布。
    
   - 平方误差函数将进一步严重恶化这种影。

#### k-中心点（基于代表对象的技术）

k-均值算法可能对离群点敏感性过高，所以可以不采用簇中对象的均值作为参照点，而是挑选实际对象来代表簇。每个簇使用一个代表对象，其余每个对象被分配到与其最为相似的代表性对象所在的簇中。划分方法基于最小化所有对象p与其对应的代表对象之间的相异度之和的原则来划分。假设有k个簇，计算各个簇中的代表对象为o<sub>i</sub>与所有对象p求绝对误差dist(p, o<sub>i</sub>)，并将所有簇的该值累加。k-中心点聚类通过最小化该绝对误差，把n个对象划分到k个簇中。

当k=1时，可以在O(n<sup>2</sup>)时间内找到准确的中位数，但k为一般正整数时，k-中心点问题是NP-Hard问题。


























